{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54b96b5c-ef0e-4525-9148-f417b1353120",
   "metadata": {},
   "source": [
    "# Random Forest Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207cfd55-7580-4b07-8a97-2c218daddfa4",
   "metadata": {},
   "source": [
    "### Why Random Forest Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafa178f-8284-4e06-aadb-a4b01738c270",
   "metadata": {},
   "source": [
    "Decision trees leave you with a difficult decision. A deep tree with lots of leaves will overfit because each prediction is coming from historical data from only the few houses at its leaf. But a shallow tree with few leaves will perform poorly because it fails to capture as many distinctions in the raw data.\n",
    "\n",
    "Even today's most sophisticated modeling techniques face this complexity between underfitting and overfitting. But, many models have clever ideas that can lead to better performance. We'll look at the random forest model as an example.\n",
    "\n",
    "The random forest uses many trees, and it makes a prediction by averaging the predictions of each component tree. It generally has much better predictive accuracy than a single decision tree and it works well with default parameters. If you keep modeling, you can learn more models with even better performance, but many of those are sensitive to getting the right parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fa0ccb4-a655-4657-9a27-07289696a471",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "    \n",
    "# Load data\n",
    "melbourne_data = pd.read_csv(\"melb_data.csv\")  \n",
    "# Filter rows with missing values\n",
    "melbourne_data = melbourne_data.dropna(axis=0)\n",
    "# Choose target and features\n",
    "y = melbourne_data.Price\n",
    "melbourne_features = ['Rooms', 'Bathroom', 'Landsize', 'BuildingArea', \n",
    "                        'YearBuilt', 'Lattitude', 'Longtitude']\n",
    "X = melbourne_data[melbourne_features]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split data into training and validation data, for both features and target\n",
    "# The split is based on a random number generator. Supplying a numeric value to\n",
    "# the random_state argument guarantees we get the same split every time we\n",
    "# run this script.\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y,random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd76438a-bb4c-4fba-92e1-b8d9c9c1c62e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191669.7536453626\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "forest_model = RandomForestRegressor(random_state=1)\n",
    "forest_model.fit(train_X, train_y)\n",
    "melb_preds = forest_model.predict(val_X)\n",
    "print(mean_absolute_error(val_y, melb_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a0db10-08fd-462f-a6db-7b1556395d23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('base': conda)",
   "language": "python",
   "name": "python3613jvsc74a57bd0bcaef06b848eec2320239edb2b676b83787ee04762e69d65ede02a0475393f93"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
